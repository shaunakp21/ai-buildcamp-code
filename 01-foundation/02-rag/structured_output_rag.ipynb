{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91180682-d709-4a24-912d-ac03ac87271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c8b7f1-02fb-4f19-9fad-a323be993be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca746471-a381-4f14-a78e-38233b9d1db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "  'date': {'title': 'Date', 'type': 'string'},\n",
       "  'participants': {'items': {'type': 'string'},\n",
       "   'title': 'Participants',\n",
       "   'type': 'array'}},\n",
       " 'required': ['name', 'date', 'participants'],\n",
       " 'title': 'CalendarEvent',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CalendarEvent.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e149e1d-1750-40a6-aca8-21e3a48184c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_client.responses.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        }\n",
    "    ],\n",
    "    text_format=CalendarEvent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eec9e34-03ca-45ca-b51b-945a39e94f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedResponse[CalendarEvent](id='resp_08d201650bca45850069a319ac04e0819684b5d90faafaa1db', created_at=1772296620.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ParsedResponseOutputMessage[CalendarEvent](id='msg_08d201650bca45850069a319ad158881969c80dd1546c62961', content=[ParsedResponseOutputText[CalendarEvent](annotations=[], text='{\"name\":\"Science Fair\",\"date\":\"Friday\",\"participants\":[\"Alice\",\"Bob\"]}', type='output_text', logprobs=[], parsed=CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob']))], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, completed_at=1772296621.0, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatTextJSONSchemaConfig(name='CalendarEvent', schema_={'properties': {'name': {'title': 'Name', 'type': 'string'}, 'date': {'title': 'Date', 'type': 'string'}, 'participants': {'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object', 'additionalProperties': False}, type='json_schema', description=None, strict=True), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=89, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=18, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=107), user=None, billing={'payer': 'developer'}, frequency_penalty=0.0, presence_penalty=0.0, store=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e9a2971-2cb1-4341-b679-41301e779b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f47f788c-9a71-4e22-a3f9-08950b5d000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\":\"Science Fair\",\"date\":\"Friday\",\"participants\":[\"Alice\",\"Bob\"]}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[0].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4500e56-4914-44bf-937c-d9075f2107fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b921365-34a9-4986-94a3-3f1859e653e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ffc0c-f6f9-474d-bb3d-fdcd2798ba43",
   "metadata": {},
   "source": [
    "# Structured RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd2c2814-6206-42a7-8853-d9896fa8acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 385 chunks from 95 documents\n"
     ]
    }
   ],
   "source": [
    "from gitsource import GithubRepositoryDataReader, chunk_documents\n",
    "from minsearch import Index\n",
    "\n",
    "reader = GithubRepositoryDataReader(\n",
    "    repo_owner=\"evidentlyai\",\n",
    "    repo_name=\"docs\",\n",
    "    allowed_extensions={\"md\", \"mdx\"},\n",
    ")\n",
    "files = reader.read()\n",
    "\n",
    "\n",
    "parsed_docs = [doc.parse() for doc in files]\n",
    "chunked_docs = chunk_documents(parsed_docs, size=3000, step=1500)\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"title\", \"description\", \"content\"],\n",
    "    keyword_fields=[\"filename\"]\n",
    ")\n",
    "index.fit(chunked_docs)\n",
    "\n",
    "print(f\"Indexed {len(chunked_docs)} chunks from {len(files)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee1c5fa8-12f7-40c6-884c-1231864a4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt, instructions)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42062f86-baff-4873-8fe1-0bdf8412c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    results = index.search(query = query, num_results=5)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2653f088-32b1-414f-a7fe-b173002cb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instructions = \"\"\"\n",
    "You're a documentation assistant. Answer the QUESTION based on the CONTEXT from our documentation.\n",
    "\n",
    "Use only facts from the CONTEXT when answering.\n",
    "If the answer isn't in the CONTEXT, say so.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results, indent=2)\n",
    "    return prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66b15434-536a-434f-b306-007a59539aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm(user_prompt,\n",
    "        instructions=None,\n",
    "        model=\"gpt-4o-mini\"\n",
    "    ):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ad75a05-de87-4e56-b84f-1805a8b6bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag('How do I implement llm as a judge',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a50135e8-7fb1-4c4b-9fdb-93d414687e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To implement LLM as a judge, follow these steps based on the tutorial provided:\\n\\n1. **Setup**:\\n   - Install the required package:\\n     ```bash\\n     pip install evidently\\n     ```\\n   - Import necessary modules:\\n     ```python\\n     import pandas as pd\\n     import numpy as np\\n     from evidently import Dataset, DataDefinition, Report, BinaryClassification\\n     from evidently.llm.templates import BinaryClassificationPromptTemplate\\n     ```\\n\\n2. **Create an Evaluation Dataset**:\\n   - Design a toy Q&A dataset that includes:\\n     - Questions as inputs.\\n     - Target responses as approved answers.\\n     - New responses imitated from the system.\\n     - Manual labels indicating if the response is correct or incorrect.\\n\\n3. **Pass OpenAI API Key**:\\n   - Set your OpenAI API key as an environment variable:\\n     ```python\\n     import os\\n     os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n     ```\\n\\n4. **Run LLM as a Judge**:\\n   - Create an LLM evaluator prompt and attach descriptors such as correctness or verbosity to evaluate the responses:\\n     ```python\\n     eval_dataset.add_descriptors(descriptors=[\\n         LLMEval(\"new_response\",\\n                 template=correctness,\\n                 provider=\"openai\",\\n                 model=\"gpt-4o-mini\",\\n                 alias=\"Correctness\",\\n                 additional_columns={\"target_response\": \"target_response\"})\\n     ])\\n     ```\\n\\n5. **Generate a Report**:\\n   - Run a report to summarize the evaluation results:\\n     ```python\\n     report = Report([TextEvals()])\\n     my_eval = report.run(eval_dataset, None)\\n     ```\\n\\n6. **Evaluate the LLM\\'s Quality**:\\n   - To assess the quality of your LLM evaluator, define a binary classification problem where you check the predictions against the manual labels.\\n\\nThis process will help you create and evaluate an LLM judge effectively. For further details and advanced configurations, consider checking the full documentation provided.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0d10deb-efb5-4bd2-8248-79d50b83e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_structured(user_prompt,\n",
    "                   output_type,\n",
    "                   instructions=None,\n",
    "                   model=\"gpt-4o-mini\"\n",
    "    ):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_type\n",
    "    )\n",
    "\n",
    "    return response.output_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9938255-5e95-473c-a9de-9e6f42d7c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_structured(user_prompt=\"Alice and Bob are going to a Science Fair on Friday\",\n",
    "                          instructions=\"Extract the event information\",\n",
    "                          output_type=CalendarEvent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98bfb854-4a07-4408-9abf-294289bf0fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad1f1aaa-1b2c-4354-9757-948f2bcd8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    answer: Optional[str]=None\n",
    "    found_answer: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c57bf98e-2df4-4656-9cdd-2337782ef750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_structured(query,output_type=RAGResponse):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    return llm_structured(user_prompt=prompt,\n",
    "                          instructions=instructions,\n",
    "                          output_type=output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ea73993-4b75-46a1-9a35-f546d31a7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_structured('How do I install Kafka?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "828b8fe7-5887-47b5-80b5-48cb6d2a01c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(answer.answer)\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f21aba2-7c40-4350-bce9-8845dc211a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'answer': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Answer'},\n",
       "  'found_answer': {'title': 'Found Answer', 'type': 'boolean'}},\n",
       " 'required': ['found_answer'],\n",
       " 'title': 'RAGResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAGResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a58fac2-5050-42d6-891f-91d65b39302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instructions = \"\"\"\n",
    "You're a documentation assistant. Answer the QUESTION based on the CONTEXT from our documentation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c23a1661-3e20-44d1-ab96-73af9e83a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGResponse(BaseModel):\n",
    "    \"\"\"   \n",
    "    The response from the RAG documentation system\n",
    "    If the answer to the question is not found in the database, 'answer' is none.\n",
    "    \"\"\"\n",
    "    answer: Optional[str]=None\n",
    "    found_answer: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ac93454-d2cc-4d32-a4dd-ebef1de34a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"The response from the RAG documentation system\\nIf the answer to the question is not found in the database, 'answer' is none.\",\n",
       " 'properties': {'answer': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Answer'},\n",
       "  'found_answer': {'title': 'Found Answer', 'type': 'boolean'}},\n",
       " 'required': ['found_answer'],\n",
       " 'title': 'RAGResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAGResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7089716d-00b2-46aa-bc12-bb9ca1d5fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "answer = rag_structured('How do I install Kafka?')\n",
    "print(answer.answer)\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74a40108-c4a7-461c-8e7f-bccdd8089b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    \"\"\"   \n",
    "    The response from the RAG documentation system\n",
    "    \"\"\"\n",
    "    answer: Optional[str]=Field(None, description=\"If you can't find the answer, set 'answer' to None\")\n",
    "    found_answer: bool =Field(description=\"True if the answer is found , False otherwise\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da25f3a6-090b-4406-93ae-a94e4390c3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'The response from the RAG documentation system',\n",
       " 'properties': {'answer': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': \"If you can't find the answer, set 'answer' to None\",\n",
       "   'title': 'Answer'},\n",
       "  'found_answer': {'description': 'True if the answer is found , False otherwise',\n",
       "   'title': 'Found Answer',\n",
       "   'type': 'boolean'}},\n",
       " 'required': ['found_answer'],\n",
       " 'title': 'RAGResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAGResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "620dbd90-5358-48f2-ad43-2c7e5e8e89fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "answer = rag_structured('How do I install Kafka?')\n",
    "print(answer.answer)\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa7a204e-6491-4d62-9d81-f12226c00c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    This model provides a structured answer with metadata about the response,\n",
    "    including confidence, categorization, and follow-up suggestions.\n",
    "    \"\"\"\n",
    "    answer: str = Field(None, description=\"The main answer to the user's question in markdown\")\n",
    "    found_answer: bool = Field(description=\"True if relevant information was found in the documentation\")\n",
    "    confidence: float = Field(description=\"Confidence score from 0.0 to 1.0 indicating how certain the answer is\")\n",
    "    confidence_explanation: str = Field(description=\"Explanation about the confidence level\")\n",
    "    answer_type: Literal[\"how-to\", \"explanation\", \"troubleshooting\", \"comparison\", \"reference\"] = Field(description=\"The category of the answer\")\n",
    "    followup_questions: list[str] = Field(description=\"Suggested follow-up questions the user might want to ask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01d1a534-8c0f-4bb1-98ee-0d58b5f0b77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'This model provides a structured answer with metadata about the response,\\nincluding confidence, categorization, and follow-up suggestions.',\n",
       " 'properties': {'answer': {'description': \"The main answer to the user's question in markdown\",\n",
       "   'title': 'Answer',\n",
       "   'type': 'string'},\n",
       "  'found_answer': {'description': 'True if relevant information was found in the documentation',\n",
       "   'title': 'Found Answer',\n",
       "   'type': 'boolean'},\n",
       "  'confidence': {'description': 'Confidence score from 0.0 to 1.0 indicating how certain the answer is',\n",
       "   'title': 'Confidence',\n",
       "   'type': 'number'},\n",
       "  'confidence_explanation': {'description': 'Explanation about the confidence level',\n",
       "   'title': 'Confidence Explanation',\n",
       "   'type': 'string'},\n",
       "  'answer_type': {'description': 'The category of the answer',\n",
       "   'enum': ['how-to',\n",
       "    'explanation',\n",
       "    'troubleshooting',\n",
       "    'comparison',\n",
       "    'reference'],\n",
       "   'title': 'Answer Type',\n",
       "   'type': 'string'},\n",
       "  'followup_questions': {'description': 'Suggested follow-up questions the user might want to ask',\n",
       "   'items': {'type': 'string'},\n",
       "   'title': 'Followup Questions',\n",
       "   'type': 'array'}},\n",
       " 'required': ['answer',\n",
       "  'found_answer',\n",
       "  'confidence',\n",
       "  'confidence_explanation',\n",
       "  'answer_type',\n",
       "  'followup_questions'],\n",
       " 'title': 'RAGResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAGResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3cb90cc-a086-4e16-8abd-36bf93850064",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_structured('How do I evaluate llms?', RAGResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d8a7c7c-3f0e-4afc-9928-1b158dced2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### How to Evaluate LLMs\n",
      "\n",
      "Evaluating Language Learning Models (LLMs) can be accomplished using a structured approach involving multiple LLMs to judge the same outputs. Below is a step-by-step guide:\n",
      "\n",
      "#### 1. **Preparation**\n",
      "   - **Install Required Libraries**:\n",
      "     ```python\n",
      "     pip install evidently litellm\n",
      "     ```\n",
      "   - **Import Necessary Components**:\n",
      "     ```python\n",
      "     import pandas as pd\n",
      "     from evidently import Dataset, DataDefinition, Report\n",
      "     from evidently.presets import TextEvals\n",
      "     from evidently.descriptors import LLMEval, TestSummary\n",
      "     from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "     ```\n",
      "\n",
      "#### 2. **Set Up Evaluator LLMs**\n",
      "   - Pass the API keys for the LLMs you'll utilize:\n",
      "     ```python\n",
      "     import os\n",
      "     os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "     os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY\"\n",
      "     os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_KEY\"\n",
      "     ```\n",
      "\n",
      "   - Optionally, set up an Evidently Cloud workspace to store evaluation results:\n",
      "     ```python\n",
      "     ws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\n",
      "     ```\n",
      "\n",
      "#### 3. **Define Evaluation Prompts**\n",
      "   - Use a `BinaryClassificationPromptTemplate` to create an evaluation guideline:\n",
      "     ```python\n",
      "     us_corp_email_appropriateness = BinaryClassificationPromptTemplate(\n",
      "         pre_messages=[(\"system\", \"You are an expert in U.S. corporate communication...\")],\n",
      "         criteria=\"An APPROPRIATE email text...\",\n",
      "         target_category=\"APPROPRIATE\",\n",
      "         non_target_category=\"INAPPROPRIATE\",\n",
      "         include_reasoning=True,\n",
      "     )\n",
      "     ```\n",
      "\n",
      "#### 4. **Create a Panel of LLM Judges**  \n",
      "   - Evaluate the outputs from multiple LLMs:\n",
      "     ```python\n",
      "     llm_evals = Dataset.from_pandas(\n",
      "         eval_df,\n",
      "         data_definition=DataDefinition(),\n",
      "         descriptors=[\n",
      "             LLMEval(\"generated email\", template=us_corp_email_appropriateness, provider=\"openai\", ...),\n",
      "             ...\n",
      "         ]\n",
      "     )\n",
      "     ```  \n",
      "\n",
      "   - Add a `TestSummary` to compute final results based on LLM approvals:\n",
      "     ```python\n",
      "     TestSummary(success_all=True, success_count=True, success_rate=True)\n",
      "     ```  \n",
      "\n",
      "#### 5. **Run and View Evaluation Reports**\n",
      "   - Generate and explore results:\n",
      "     ```python\n",
      "     report = Report([\n",
      "         TextEvals()\n",
      "     ])\n",
      "     my_eval = report.run(llm_evals, None)\n",
      "     ```  \n",
      "   - You can also upload results to the Evidently Cloud or view them locally:\n",
      "     ```python\n",
      "     ws.add_run(project.id, my_eval, include_data=True)\n",
      "     my_eval\n",
      "     ```  \n",
      "\n",
      "### Summary\n",
      "This structured evaluation approach allows you to assess LLM outputs and gain insights through the consensus or disagreements among different models.\n",
      "True\n",
      "0.95\n",
      "['What metrics should I use in LLM evaluation?', 'Can I use other LLMs for evaluation aside from the ones mentioned?', 'What are common pitfalls in LLM evaluation?']\n"
     ]
    }
   ],
   "source": [
    "print(answer.answer)\n",
    "print(answer.found_answer)\n",
    "print(answer.confidence)\n",
    "print(answer.followup_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "608d6b8f-2647-418b-8320-66b5370a8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_structured('How do I install Kafka on windows?', RAGResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "202e8b4e-446e-4899-8772-9470e41670f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install Kafka on Windows, follow these steps:\n",
      "\n",
      "1. **Download Kafka**: Visit the [Apache Kafka web\n",
      "False\n",
      "0.9\n",
      "There was no specific documentation in the provided context about installing Kafka on Windows, but the information provided here is commonly known and likely accurate based on generally available resources.\n",
      "['What is Kafka used for?', 'How do I configure Kafka settings?', 'Can I run Kafka on a cloud service?']\n"
     ]
    }
   ],
   "source": [
    "print(answer.answer[:100])\n",
    "print(answer.found_answer)\n",
    "print(answer.confidence)\n",
    "print(answer.confidence_explanation)\n",
    "print(answer.followup_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae261495-2691-4aaf-adda-6a6058e05453",
   "metadata": {},
   "source": [
    "# Nested Fields with Mutual Exclusivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a93f0cba-31f9-4a59-93ac-118ac852630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import model_validator\n",
    "\n",
    "class AnswerNotFound(BaseModel):\n",
    "    explanation: str\n",
    "\n",
    "class AnswerResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    If answer is found, 'answer' is populated.\n",
    "    If no answer is found, 'answer_not_found' is populated.\n",
    "    Only one of the two fields can be set at a time. Never both or neither.\n",
    "    \"\"\"\n",
    "\n",
    "    answer_not_found: Optional[AnswerNotFound] = None\n",
    "    found_answer: bool\n",
    "    answer: Optional[RAGResponse] = None\n",
    "\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_consistency(self):\n",
    "        if self.answer is not None and self.answer_not_found is not None:\n",
    "            raise ValueError(\"Provide either 'answer' or 'answer_not_found', not both.\")\n",
    "\n",
    "        if self.answer is None and self.answer_not_found is None:\n",
    "            raise ValueError(\"Provide either 'answer' or 'answer_not_found'.\")\n",
    "\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c413306-e271-47c8-9fab-273cfe3951e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AnswerNotFound': {'properties': {'explanation': {'title': 'Explanation',\n",
       "     'type': 'string'}},\n",
       "   'required': ['explanation'],\n",
       "   'title': 'AnswerNotFound',\n",
       "   'type': 'object'},\n",
       "  'RAGResponse': {'description': 'This model provides a structured answer with metadata about the response,\\nincluding confidence, categorization, and follow-up suggestions.',\n",
       "   'properties': {'answer': {'description': \"The main answer to the user's question in markdown\",\n",
       "     'title': 'Answer',\n",
       "     'type': 'string'},\n",
       "    'found_answer': {'description': 'True if relevant information was found in the documentation',\n",
       "     'title': 'Found Answer',\n",
       "     'type': 'boolean'},\n",
       "    'confidence': {'description': 'Confidence score from 0.0 to 1.0 indicating how certain the answer is',\n",
       "     'title': 'Confidence',\n",
       "     'type': 'number'},\n",
       "    'confidence_explanation': {'description': 'Explanation about the confidence level',\n",
       "     'title': 'Confidence Explanation',\n",
       "     'type': 'string'},\n",
       "    'answer_type': {'description': 'The category of the answer',\n",
       "     'enum': ['how-to',\n",
       "      'explanation',\n",
       "      'troubleshooting',\n",
       "      'comparison',\n",
       "      'reference'],\n",
       "     'title': 'Answer Type',\n",
       "     'type': 'string'},\n",
       "    'followup_questions': {'description': 'Suggested follow-up questions the user might want to ask',\n",
       "     'items': {'type': 'string'},\n",
       "     'title': 'Followup Questions',\n",
       "     'type': 'array'}},\n",
       "   'required': ['answer',\n",
       "    'found_answer',\n",
       "    'confidence',\n",
       "    'confidence_explanation',\n",
       "    'answer_type',\n",
       "    'followup_questions'],\n",
       "   'title': 'RAGResponse',\n",
       "   'type': 'object'}},\n",
       " 'description': \"If answer is found, 'answer' is populated.\\nIf no answer is found, 'answer_not_found' is populated.\\nOnly one of the two fields can be set at a time. Never both or neither.\",\n",
       " 'properties': {'answer_not_found': {'anyOf': [{'$ref': '#/$defs/AnswerNotFound'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None},\n",
       "  'found_answer': {'title': 'Found Answer', 'type': 'boolean'},\n",
       "  'answer': {'anyOf': [{'$ref': '#/$defs/RAGResponse'}, {'type': 'null'}],\n",
       "   'default': None}},\n",
       " 'required': ['found_answer'],\n",
       " 'title': 'AnswerResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnswerResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "afd4aaa3-fa15-491f-9dd0-2332e8d8c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_structured('How do I install Kafka on windows?', AnswerResponse)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12918b59-6cd3-4ce9-b457-348a8cb8d85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerResponse(answer_not_found=None, found_answer=True, answer=RAGResponse(answer='## How to Evaluate LLMs\\n\\nEvaluating Large Language Models (LLMs) can be approached through a method involving multiple LLMs acting as judges to assess the same output. This method provides a more reliable evaluation by consolidating their assessments to determine if an output is appropriate or if there are differing opinions. Hereâ€™s a structured approach:\\n\\n### Step-by-Step Evaluation Process\\n\\n1. **Preparation**\\n   - Install Evidently and necessary packages:\\n     ```bash\\n     pip install evidently litellm\\n     ```\\n   - Import required components:\\n     ```python\\n     import pandas as pd\\n     from evidently import Dataset, DataDefinition, Report\\n     from evidently.presets import TextEvals\\n     from evidently.descriptors import LLMEval, TestSummary\\n     ```\\n\\n2. **Set Up Evaluator LLMs**\\n   - Provide API keys for the LLMs you will use:\\n     ```python\\n     import os\\n     os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n     os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY\"\\n     os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_KEY\"\\n     ```\\n   - Optionally, set up an Evidently Cloud workspace.\\n\\n3. **Define Evaluation Prompts**\\n   - Create a `BinaryClassificationPromptTemplate` to instruct the LLMs on what criteria to use for judging the outputs. Define what constitutes an appropriate or inappropriate email.\\n\\n4. **Create a Panel of LLM Judges**\\n   - Set up evaluations using different LLMs:\\n     ```python\\n     llm_evals = Dataset.from_pandas(eval_df, data_definition=DataDefinition(), descriptors=[\\n         LLMEval(...),  # Add LLMs along with their evaluation templates\\n         TestSummary(...)\\n     ])\\n     ```\\n\\n5. **Run Evaluations and View Reports**\\n   - Evaluate the outputs and generate a report using:\\n     ```python\\n     report = Report([\\n         TextEvals()\\n     ])\\n     my_eval = report.run(llm_evals, None)\\n     ```\\n   - You can then upload results to the Evidently Cloud or visualize locally.\\n\\n### Example Evaluation Code Snippet\\n```python\\n# Define the data to evaluate\\ndata = [[\"User input\", \"Generated content\"]]\\neval_df = pd.DataFrame(data, columns=[\"user input\", \"generated email\"])\\n\\n# Run evaluations\\nllm_evals = Dataset.from_pandas(eval_df, data_definition=DataDefinition(), descriptors=[\\n    LLMEval(...),  # LLM specifics go here\\n])\\n\\n# Generate report\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(llm_evals, None)\\n```  \\n\\n### Additional Resources\\n- For more details, read about the concept of using LLMs as judges in this [blog post](https://www.evidentlyai.com/blog/llm-judges-jury) and explore code examples [here](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_jury_Example.ipynb).', found_answer=True, confidence=0.95, confidence_explanation='The information provided is directly derived from the documented approach to evaluating LLMs using multiple judges and detailed steps for implementation.', answer_type='how-to', followup_questions=['What types of outputs can I evaluate with LLMs?', 'How do I refine the evaluation criteria for LLMs?', 'Can I integrate other LLMs into this evaluation framework?']))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = rag_structured('How do I evaluate llms?', AnswerResponse)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f35e86b-67ef-4224-8e2e-bc733b5b5e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error\n",
      "1 validation error for AnswerResponse\n",
      "found_answer\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "try:\n",
    "    AnswerResponse()\n",
    "except ValidationError as e:\n",
    "    print(\"Validation Error\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c06fc9-9bab-4c2b-86ff-b7283c8c61ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
