{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4001bb52-6bcb-4949-afaf-6f5fece21a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m113 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m110 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6dd69d-4116-42b9-9ea3-f572ce49cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313d5187-6834-41db-897b-971e1493728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_owner = 'evidentlyai'\n",
    "repo_name = 'docs'\n",
    "branch_name = 'main'\n",
    "\n",
    "zip_url = f'https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/{branch_name}.zip'\n",
    "zip_response = requests.get(zip_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4ca169-79bd-49c5-9271-bfaaa6249c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17545668"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zip_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e43575e7-83e7-4678-9f75-c2492a2abb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "\n",
    "zip_archive = zipfile.ZipFile(io.BytesIO(zip_response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c059ce01-20a7-4ccf-94e0-f1e7f0d39e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs-main/',\n",
       " 'docs-main/api-reference/',\n",
       " 'docs-main/api-reference/endpoint/',\n",
       " 'docs-main/api-reference/endpoint/create.mdx',\n",
       " 'docs-main/api-reference/endpoint/delete.mdx',\n",
       " 'docs-main/api-reference/endpoint/get.mdx',\n",
       " 'docs-main/api-reference/introduction.mdx',\n",
       " 'docs-main/api-reference/openapi.json',\n",
       " 'docs-main/changelog/',\n",
       " 'docs-main/changelog/changelog.mdx',\n",
       " 'docs-main/docs/',\n",
       " 'docs-main/docs/library/',\n",
       " 'docs-main/docs/library/data_definition.mdx',\n",
       " 'docs-main/docs/library/descriptors.mdx',\n",
       " 'docs-main/docs/library/evaluations_overview.mdx',\n",
       " 'docs-main/docs/library/leftover_content.mdx',\n",
       " 'docs-main/docs/library/metric_generator.mdx',\n",
       " 'docs-main/docs/library/output_formats.mdx',\n",
       " 'docs-main/docs/library/overview.mdx',\n",
       " 'docs-main/docs/library/prompt_optimization.mdx',\n",
       " 'docs-main/docs/library/report.mdx',\n",
       " 'docs-main/docs/library/synthetic_data_api.mdx',\n",
       " 'docs-main/docs/library/tags_metadata.mdx',\n",
       " 'docs-main/docs/library/tests.mdx',\n",
       " 'docs-main/docs/platform/',\n",
       " 'docs-main/docs/platform/alerts.mdx',\n",
       " 'docs-main/docs/platform/dashboard_add_panels.mdx',\n",
       " 'docs-main/docs/platform/dashboard_add_panels_ui.mdx',\n",
       " 'docs-main/docs/platform/dashboard_overview.mdx',\n",
       " 'docs-main/docs/platform/dashboard_panel_types.mdx',\n",
       " 'docs-main/docs/platform/datasets_generate.mdx',\n",
       " 'docs-main/docs/platform/datasets_overview.mdx',\n",
       " 'docs-main/docs/platform/datasets_workflow.mdx',\n",
       " 'docs-main/docs/platform/evals_api.mdx',\n",
       " 'docs-main/docs/platform/evals_explore.mdx',\n",
       " 'docs-main/docs/platform/evals_no_code.mdx',\n",
       " 'docs-main/docs/platform/evals_overview.mdx',\n",
       " 'docs-main/docs/platform/monitoring_local_batch.mdx',\n",
       " 'docs-main/docs/platform/monitoring_overview.mdx',\n",
       " 'docs-main/docs/platform/monitoring_scheduled_evals.mdx',\n",
       " 'docs-main/docs/platform/overview.mdx',\n",
       " 'docs-main/docs/platform/projects_manage.mdx',\n",
       " 'docs-main/docs/platform/projects_overview.mdx',\n",
       " 'docs-main/docs/platform/tracing_overview.mdx',\n",
       " 'docs-main/docs/platform/tracing_setup.mdx',\n",
       " 'docs-main/docs/setup/',\n",
       " 'docs-main/docs/setup/cloud.mdx',\n",
       " 'docs-main/docs/setup/installation.mdx',\n",
       " 'docs-main/docs/setup/self-hosting.mdx',\n",
       " 'docs-main/examples/',\n",
       " 'docs-main/examples/GitHub_actions.mdx',\n",
       " 'docs-main/examples/LLM_evals.mdx',\n",
       " 'docs-main/examples/LLM_judge.mdx',\n",
       " 'docs-main/examples/LLM_jury.mdx',\n",
       " 'docs-main/examples/LLM_rag_evals.mdx',\n",
       " 'docs-main/examples/LLM_regression_testing.mdx',\n",
       " 'docs-main/examples/introduction.mdx',\n",
       " 'docs-main/faq/',\n",
       " 'docs-main/faq/cloud_v2.mdx',\n",
       " 'docs-main/faq/contact.mdx',\n",
       " 'docs-main/faq/introduction.mdx',\n",
       " 'docs-main/faq/migration.mdx',\n",
       " 'docs-main/faq/oss_vs_cloud.mdx',\n",
       " 'docs-main/faq/telemetry.mdx',\n",
       " 'docs-main/faq/why_evidently.mdx',\n",
       " 'docs-main/images/',\n",
       " 'docs-main/images/alerts.png',\n",
       " 'docs-main/images/changelog/',\n",
       " 'docs-main/images/changelog/editable_dataset-min.png',\n",
       " 'docs-main/images/changelog/readme.md',\n",
       " 'docs-main/images/concepts/',\n",
       " 'docs-main/images/concepts/evidently_oss_ui-min.png',\n",
       " 'docs-main/images/concepts/overview_descriptor_test_example-min.png',\n",
       " 'docs-main/images/concepts/overview_descriptors_export.png',\n",
       " 'docs-main/images/concepts/overview_drift_report-min.png',\n",
       " 'docs-main/images/concepts/overview_reports-min.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_cat_value_compare_example.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_cat_value_example.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_correlation_example-min.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_descriptor_example.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_num_value_example-min.png',\n",
       " 'docs-main/images/concepts/overview_test_example-min.png',\n",
       " 'docs-main/images/concepts/overview_test_suite_example-min.png',\n",
       " 'docs-main/images/concepts/readme.md',\n",
       " 'docs-main/images/concepts/report_test_preview.gif',\n",
       " 'docs-main/images/concepts/text_data_drift_domain_classifier.png',\n",
       " 'docs-main/images/dashboard/',\n",
       " 'docs-main/images/dashboard/add_dashboard_tab.gif',\n",
       " 'docs-main/images/dashboard/add_dashboard_tab_v2.gif',\n",
       " 'docs-main/images/dashboard/add_panel_ui.png',\n",
       " 'docs-main/images/dashboard/add_panel_ui_pie.png',\n",
       " 'docs-main/images/dashboard/dashboard_to_report.gif',\n",
       " 'docs-main/images/dashboard/distribution_panels.png',\n",
       " 'docs-main/images/dashboard/metric_panels.png',\n",
       " 'docs-main/images/dashboard/panel_bar_plot_example.png',\n",
       " 'docs-main/images/dashboard/panel_counter_example-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_group_2-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_overlay-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_relative-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_stacked_2-min.png',\n",
       " 'docs-main/images/dashboard/panel_hist_example.png',\n",
       " 'docs-main/images/dashboard/panel_line_chart.png',\n",
       " 'docs-main/images/dashboard/panel_line_plot_example.png',\n",
       " 'docs-main/images/dashboard/panel_pie_chart.png',\n",
       " 'docs-main/images/dashboard/panel_scatter_plot_example.png',\n",
       " 'docs-main/images/dashboard/panel_tests_aggregated_hover_example.png',\n",
       " 'docs-main/images/dashboard/panel_tests_counter_example.png',\n",
       " 'docs-main/images/dashboard/panel_tests_detailed_hover_example.png',\n",
       " 'docs-main/images/dashboard/readme.md',\n",
       " 'docs-main/images/dashboard/test_panels.png',\n",
       " 'docs-main/images/dashboard_llm_dark.png',\n",
       " 'docs-main/images/dashboard_llm_light.png',\n",
       " 'docs-main/images/dashboard_llm_tabs.gif',\n",
       " 'docs-main/images/dataset_llm.png',\n",
       " 'docs-main/images/datasets_input_data_two.png',\n",
       " 'docs-main/images/evals_browse_reports-min.png',\n",
       " 'docs-main/images/evals_explore_view-min.png',\n",
       " 'docs-main/images/evals_flow_nocode.png',\n",
       " 'docs-main/images/evals_flow_python.png',\n",
       " 'docs-main/images/evals_no_code_add_descriptors-min.png',\n",
       " 'docs-main/images/examples/',\n",
       " 'docs-main/images/examples/dashboard_quickstart.png',\n",
       " 'docs-main/images/examples/data_drift_quickstart.png',\n",
       " 'docs-main/images/examples/github_actions.gif',\n",
       " 'docs-main/images/examples/hf_descriptor_example_toxicity-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_appropriate_question-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_context_quality-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_hallucination-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_multi_class_relevance.png',\n",
       " 'docs-main/images/examples/llm_judge_example_multi_class_safety.png',\n",
       " 'docs-main/images/examples/llm_judge_example_toxicity-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_cloud-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_conf_matrix-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_data_preview-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_judge_label_dist-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_judge_scored_data-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_report-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_verbosity-min.png',\n",
       " 'docs-main/images/examples/llm_jury_example.png',\n",
       " 'docs-main/images/examples/llm_jury_overview.png',\n",
       " 'docs-main/images/examples/llm_quickstart_create_tab.gif',\n",
       " 'docs-main/images/examples/llm_quickstart_create_tab_new.gif',\n",
       " 'docs-main/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png',\n",
       " 'docs-main/images/examples/llm_quickstart_descriptor_tests-min.png',\n",
       " 'docs-main/images/examples/llm_quickstart_descriptor_tests_report-min.png',\n",
       " 'docs-main/images/examples/llm_quickstart_edit.png',\n",
       " 'docs-main/images/examples/llm_quickstart_explore.png',\n",
       " 'docs-main/images/examples/llm_quickstart_preview.png',\n",
       " 'docs-main/images/examples/llm_quickstart_report.png',\n",
       " 'docs-main/images/examples/llm_quickstart_tests.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_dashboard-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_data_preview-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_data_stats-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_new_data-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_report1-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_scored-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_stats_report-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_style-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_tests1-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_tests2-min.png',\n",
       " 'docs-main/images/examples/rag_cloud_view-min.png',\n",
       " 'docs-main/images/examples/rag_correctness-min.png',\n",
       " 'docs-main/images/examples/rag_faithfulness-min.png',\n",
       " 'docs-main/images/examples/rag_multi_context_hit-min.png',\n",
       " 'docs-main/images/examples/rag_multi_context_mean-min.png',\n",
       " 'docs-main/images/examples/rag_reports-min.png',\n",
       " 'docs-main/images/examples/rag_single_context_hit-min.png',\n",
       " 'docs-main/images/examples/rag_single_context_valid-min.png',\n",
       " 'docs-main/images/examples/rag_tests-min.png',\n",
       " 'docs-main/images/examples/readme.md',\n",
       " 'docs-main/images/examples/tracing_tutorial_dataset_view.png',\n",
       " 'docs-main/images/examples/tracing_tutorial_evals.png',\n",
       " 'docs-main/images/examples/tracing_tutorial_session_view.png',\n",
       " 'docs-main/images/examples/tracing_tutorial_traces_view.png',\n",
       " 'docs-main/images/library_small-min.png',\n",
       " 'docs-main/images/metrics/',\n",
       " 'docs-main/images/metrics/descriptors-min.png',\n",
       " 'docs-main/images/metrics/descriptors-report-test.png',\n",
       " 'docs-main/images/metrics/descriptors-report.png',\n",
       " 'docs-main/images/metrics/descriptors_tests-min.png',\n",
       " 'docs-main/images/metrics/preset_classification-min.png',\n",
       " 'docs-main/images/metrics/preset_classification_2-min.png',\n",
       " 'docs-main/images/metrics/preset_classification_example-min.png',\n",
       " 'docs-main/images/metrics/preset_data_drift-min.png',\n",
       " 'docs-main/images/metrics/preset_data_drift_2-min.png',\n",
       " 'docs-main/images/metrics/preset_data_drift_3-min.png',\n",
       " 'docs-main/images/metrics/preset_dataset_summary-min.png',\n",
       " 'docs-main/images/metrics/preset_datasummary_example-min.png',\n",
       " 'docs-main/images/metrics/preset_recsys-min.png',\n",
       " 'docs-main/images/metrics/preset_recsys_2-min.png',\n",
       " 'docs-main/images/metrics/preset_regression-min.png',\n",
       " 'docs-main/images/metrics/preset_regression_2-min.png',\n",
       " 'docs-main/images/metrics/preset_text_evals-min.gif',\n",
       " 'docs-main/images/metrics/preset_value_stats-min.png',\n",
       " 'docs-main/images/metrics/readme.md',\n",
       " 'docs-main/images/metrics/test_preset_classification-min.png',\n",
       " 'docs-main/images/metrics/test_preset_data_drift-min.png',\n",
       " 'docs-main/images/metrics/test_preset_dataset_summary-min.png',\n",
       " 'docs-main/images/metrics/test_preset_recsys-min.png',\n",
       " 'docs-main/images/metrics/test_preset_regression-min.png',\n",
       " 'docs-main/images/monitoring_batch_workflow_min.png',\n",
       " 'docs-main/images/monitoring_flow_batch.png',\n",
       " 'docs-main/images/monitoring_flow_tracing.png',\n",
       " 'docs-main/images/nocode_choose_evals-min.png',\n",
       " 'docs-main/images/nocode_column_mapping-min.png',\n",
       " 'docs-main/images/nocode_includes_words-min.png',\n",
       " 'docs-main/images/nocode_judge_result-min.png',\n",
       " 'docs-main/images/nocode_llm_judge-min.png',\n",
       " 'docs-main/images/nocode_semantic_similarity-min.png',\n",
       " 'docs-main/images/nocode_start_eval-min.png',\n",
       " 'docs-main/images/platform_compare_select.png',\n",
       " 'docs-main/images/platform_compare_view.png',\n",
       " 'docs-main/images/platform_small-min.png',\n",
       " 'docs-main/images/projects.png',\n",
       " 'docs-main/images/synth_data-min.png',\n",
       " 'docs-main/images/synthetic/',\n",
       " 'docs-main/images/synthetic/datagen_travel.gif',\n",
       " 'docs-main/images/synthetic/readme.md',\n",
       " 'docs-main/images/synthetic/synthetic_adversarial_img.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_adversarial.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_brand_image.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_forbidden.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_prompt.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_result.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_upload.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_upload2.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_rag_example_result.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_select_method.png',\n",
       " 'docs-main/images/synthetic/synthetic_experiments_img.png',\n",
       " 'docs-main/images/test_suite_dashboard-min.png',\n",
       " 'docs-main/images/tracing.png',\n",
       " 'docs-main/introduction.mdx',\n",
       " 'docs-main/logo/',\n",
       " 'docs-main/logo/evidently_ai_logo_docs.png',\n",
       " 'docs-main/logo/evidently_ai_logo_docs.svg',\n",
       " 'docs-main/logo/evidently_ai_logo_docs_dark.png',\n",
       " 'docs-main/logo/evidently_ai_logo_docs_dark.svg',\n",
       " 'docs-main/logo/favicon.png',\n",
       " 'docs-main/logo/favicon.svg',\n",
       " 'docs-main/main_img',\n",
       " 'docs-main/metrics/',\n",
       " 'docs-main/metrics/all_descriptors.mdx',\n",
       " 'docs-main/metrics/all_metrics.mdx',\n",
       " 'docs-main/metrics/all_presets.mdx',\n",
       " 'docs-main/metrics/customize_add_text.mdx',\n",
       " 'docs-main/metrics/customize_colors.mdx',\n",
       " 'docs-main/metrics/customize_data_drift.mdx',\n",
       " 'docs-main/metrics/customize_descriptor.mdx',\n",
       " 'docs-main/metrics/customize_embedding_drift.mdx',\n",
       " 'docs-main/metrics/customize_hf_descriptor.mdx',\n",
       " 'docs-main/metrics/customize_llm_judge.mdx',\n",
       " 'docs-main/metrics/customize_metric.mdx',\n",
       " 'docs-main/metrics/explainer_classification.mdx',\n",
       " 'docs-main/metrics/explainer_data_stats.mdx',\n",
       " 'docs-main/metrics/explainer_drift.mdx',\n",
       " 'docs-main/metrics/explainer_llm_evals.mdx',\n",
       " 'docs-main/metrics/explainer_recsys.mdx',\n",
       " 'docs-main/metrics/explainer_regression.mdx',\n",
       " 'docs-main/metrics/introduction.mdx',\n",
       " 'docs-main/metrics/preset_classification.mdx',\n",
       " 'docs-main/metrics/preset_data_drift.mdx',\n",
       " 'docs-main/metrics/preset_data_summary.mdx',\n",
       " 'docs-main/metrics/preset_recsys.mdx',\n",
       " 'docs-main/metrics/preset_regression.mdx',\n",
       " 'docs-main/metrics/preset_text_evals.mdx',\n",
       " 'docs-main/mint.json',\n",
       " 'docs-main/quickstart_llm.mdx',\n",
       " 'docs-main/quickstart_ml.mdx',\n",
       " 'docs-main/quickstart_tracing.mdx',\n",
       " 'docs-main/snippets/',\n",
       " 'docs-main/snippets/cloud_signup.mdx',\n",
       " 'docs-main/snippets/create_project.mdx',\n",
       " 'docs-main/synthetic-data/',\n",
       " 'docs-main/synthetic-data/adversarial_data.mdx',\n",
       " 'docs-main/synthetic-data/input_data.mdx',\n",
       " 'docs-main/synthetic-data/introduction.mdx',\n",
       " 'docs-main/synthetic-data/rag_data.mdx',\n",
       " 'docs-main/synthetic-data/why_synthetic.mdx']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = zip_archive.namelist()\n",
    "filenames[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "608c0005-39b9-4623-9519-41e1d5f05614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: 'Alerts'\n",
      "description: 'How to set up alerts.'\n",
      "---\n",
      "\n",
      "<Check>\n",
      "  Built-in alerting is a Pro feature available in the **Evidently Cloud** and **\n"
     ]
    }
   ],
   "source": [
    "filename = 'docs-main/docs/platform/alerts.mdx'\n",
    "mdx_file = zip_archive.open(filename)\n",
    "mdx_content = mdx_file.read().decode('utf-8')\n",
    "print(mdx_content[:150])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af510452-b384-45e2-8f68-5f9323cec1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Check>\n",
      "  Built-in alerting is a Pro feature available in the **Evidently Cloud** and **Evidently En\n"
     ]
    }
   ],
   "source": [
    "import frontmatter\n",
    "\n",
    "post = frontmatter.loads(mdx_content)\n",
    "print(post.content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed1ee2a-f112-4ec7-a72e-e956949202db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/platform/alerts.mdx\n"
     ]
    }
   ],
   "source": [
    "_, filename_corrected = filename.split('/',maxsplit=1)\n",
    "print(filename_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf4edb4-a664-41c5-83cb-837c0a2f34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = {\n",
    "    'content':post.content,\n",
    "    'title':post.metadata.get('title'),\n",
    "    'description':post.metadata.get('description'),\n",
    "    'filename':filename_corrected\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa1597a8-b145-4391-baae-1c97f4acccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '<Check>\\n  Built-in alerting is a Pro feature available in the **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\n![](/images/alerts.png)\\n\\nTo enable alerts, open the Project and navigate to the \"Alerts\" in the left menu. You must set:\\n\\n* A notification channel.\\n\\n* An alert condition.\\n\\n## Notification channels\\n\\nYou can choose between the following options:\\n\\n* **Email**. Add email addresses to send alerts to.\\n\\n* **Slack**. Add a Slack webhook.\\n\\n* **Discord**. Add a Discord webhook.\\n\\n## Alert conditions\\n\\n### Failed tests\\n\\nIf you use Tests (conditional checks) in your Project, you can tie alerting to the failed Tests in a Test Suite. Toggle this option on the Alerts page. Evidently will set an alert to the defined channel if any of the Tests fail.\\n\\n<Tip>\\n  **How to avoid alert fatigue?** Use the `is_critical` parameter to mark non-critical Test as Warnings. Setting it to `False` prevent alerts for those checks even if they fail.\\n</Tip>\\n\\n### Custom conditions\\n\\nYou can also set alerts on individual Metric values. For example, you can generate Alerts when the share of drifting features is above a certain threshold.\\n\\nClick on the plus sign below the “Add new Metric alert” and follow the prompts to set an alert condition.\\n\\n![](../.gitbook/assets/cloud/alerts.png)',\n",
       " 'title': 'Alerts',\n",
       " 'description': 'How to set up alerts.',\n",
       " 'filename': 'docs/platform/alerts.mdx'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa93231b-947b-4851-8179-2c88a2019af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(zip_response.content)) as zip_ref:\n",
    "    for file_path in zip_ref.namelist():\n",
    "        if not file_path.endswith(('.md', '.mdx')):\n",
    "            continue\n",
    "        with zip_ref.open(file_path) as file:\n",
    "            content = file.read().decode('utf-8')\n",
    "            post = frontmatter.loads(content)\n",
    "            doc = {\n",
    "                'content': post.content,\n",
    "                'title': post.metadata.get('title'),\n",
    "                'description': post.metadata.get('description'),\n",
    "                'filename': file_path.split('/', 1)[-1]\n",
    "            }\n",
    "            documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "464d4c6e-b3ec-40aa-9726-06f7626a9c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0cc2ce-b8e0-4cc6-bdb0-b5fc0b2d3cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m113 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m110 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add gitsource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ff80a84-19b5-43e0-87c5-ad1727e4a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95 documents\n"
     ]
    }
   ],
   "source": [
    "from gitsource import GithubRepositoryDataReader\n",
    "\n",
    "reader = GithubRepositoryDataReader(\n",
    "    repo_owner=\"evidentlyai\",\n",
    "    repo_name=\"docs\",\n",
    "    allowed_extensions={\"md\", \"mdx\"},\n",
    ")\n",
    "\n",
    "files = reader.read()\n",
    "\n",
    "print(f\"Loaded {len(files)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a202d187-eb79-492f-9be0-0e49ba046025",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = files[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a4e2d28-a90e-421c-a524-e1ab03841f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RawRepositoryFile(filename='docs/library/output_formats.mdx', content='---\\ntitle: \\'Output formats\\'\\ndescription: \\'How to export the evaluation results.\\'\\n---\\n\\nYou can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesn’t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb8b531-6d92-45e1-97d5-32cf7896156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Output formats',\n",
       " 'description': 'How to export the evaluation results.',\n",
       " 'content': 'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesn’t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```',\n",
       " 'filename': 'docs/library/output_formats.mdx'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_file.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38f7d4e9-da22-4b78-86a3-349ce0bc837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [f.parse() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c279c8e6-fe45-4564-8c1c-6cc6a070d7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e88a39f2-817e-4e18-942d-b9ed2372d3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Introduction',\n",
       " 'description': 'Example section for showcasing API endpoints',\n",
       " 'content': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       " 'filename': 'api-reference/introduction.mdx'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef19df-a39b-4590-a97b-b1e733c7f756",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0347ae49-99e5-4272-86c2-b1d07a782b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='LLM as a judge'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765de8cc-a0e9-49ee-b4db-78569b9ba431",
   "metadata": {},
   "source": [
    "rag:\n",
    "\n",
    "1) search <<--\n",
    "2) prompt\n",
    "3) llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "686a5ddb-3f36-4da2-a75b-f41e4461624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m120 packages\u001b[0m \u001b[2min 672ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m7 packages\u001b[0m \u001b[2min 747ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m7 packages\u001b[0m \u001b[2min 72ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mminsearch\u001b[0m\u001b[2m==0.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbc2234c-acf8-4eee-ac1a-19e4c93e24f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1252a57f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"title\", \"description\", \"content\"],\n",
    "    keyword_fields=[\"filename\"]\n",
    ")\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c52a2c3-9134-4309-8110-160bed700dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I add another user to my account?\",\\n     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\\n     \"To add a secondary user, go to \\'Account Settings\\', select \\'Manage Users\\', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\\n     \"incorrect\", \"contradiction (incorrect answer)\"],\\n  \\n    [\"Is it possible to link multiple bank accounts?\",\\n     \"Yes, you can link multiple bank accounts by going to \\'Account Settings\\' in the menu and selecting \\'Add Bank Account\\'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\\n     \"You can add multiple bank accounts by visiting \\'Accounts\\' in the menu and choosing \\'Add Bank Account\\'. Enter your bank details as prompted and complete the verification process for each account to link them successfully.\",\\n     \"incorrect\", \"contradiction (incorrect menu item)\"],\\n  \\n    [\"Can I use your service for cryptocurrency transactions?\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Hi, can I get a detailed report of my monthly transactions?\",\\n     \"Yes, you can generate a detailed monthly report of your transactions by logging into your account, going to \\'Transaction History\\', and selecting \\'Generate Report\\'. You can customize the report by selecting specific dates or transaction types.\",\\n     \"You can get a detailed monthly report by logging into your account, navigating to \\'Transaction History\\', and clicking on \\'Generate Report\\'. Customize your report by choosing the date range and types of transactions you want to include.\",\\n     \"correct\", \"\"],\\n  \\n    [\"I am traveling to the US. Can I use the app there?\",\\n     \"Yes, you can use the app in the US just like you do at home. Ensure you have an internet connection. You may also want to update your app to the latest version before traveling for optimal performance.\",\\n     \"The app will work in the US without any issues. Just make sure you have access to the internet. For the best experience, update your app to the latest version before you travel.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I link my payment account to a new mobile number?\",\\n     \"To link a new mobile number, log in to your account, go to \\'Account Settings\\', select \\'Mobile Number\\', and follow the instructions to verify your new number. You will need to enter the new number and verify it via a code sent to your phone.\",\\n     \"To add a new number, navigate to the \\'Account Settings\\' section, select \\'Mobile Number\\' and proceed with the steps to add and confirm the new number. Enter the new mobile number and verify it using the code sent to your phone.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Can I receive notifications for transactions in real-time?\",\\n     \"Yes, you can enable real-time notifications for transactions by going to \\'Account Settings\\', then \\'Notifications\\', and turning on \\'Transaction Alerts\\'. You can choose to receive alerts via SMS, email, or push notifications on your mobile device.\",\\n     \"To receive real-time notifications for transactions, log into your account, go to \\'Account Settings\\', select \\'Notifications\\', and enable \\'Transaction Alerts\\'. Choose your preferred notification method between email or push notifications.\",\\n     \"incorrect\", \"omits information (sms notification)\"],\\n  \\n    [\"Hey, can I set up automatic transfers to my savings account?\",\\n     \"Yes, you can set up automatic transfers by going to \\'Account Settings\\', selecting \\'Automatic Transfers\\', and specifying the amount and frequency. You can choose to transfer weekly, bi-weekly, or monthly. Make sure to save the settings to activate the transfers.\",\\n     \"You can arrange automatic transfers by going to \\'Account Settings\\', choosing \\'Automatic Transfers\\', and setting the desired amount and frequency. Don\\'t forget to save the changes to enable the automatic transfers.\",\\n     \"incorrect\", \"omits information (limited frequency of transfers available)\"],\\n  \\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How can I update my billing address?\",\\n     \"To update your billing address, log into your account, go to \\'Account Settings\\', select \\'Billing Information\\', and enter your new address. Make sure to save the changes once you are done.\",\\n     \"To update your billing address, log into your account, navigate to \\'Account Settings\\', and select \\'Billing Information\\'. Enter your new address and ensure all fields are filled out correctly. Save the changes, and you will receive a confirmation email with the updated address details.\",\\n     \"incorrect\", \"adds new information (confirmation email)\"],\\n  \\n    [\"How do I contact customer support?\",\\n     \"You can contact customer support by logging into your account, going to the \\'Help\\' section, and selecting \\'Contact Us\\'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\\n     \"To contact customer support, log into your account and go to the \\'Help\\' section. Select \\'Contact Us\\' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\\n     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\\n  \\n    [\"What should I do if my card is lost or stolen?\",\\n     \"If your card is lost or stolen, immediately log into your account, go to \\'Card Management\\', and select \\'Report Lost/Stolen\\'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\\n     \"If your card is lost or stolen, navigate to \\'Card Management\\' in your account, and select \\'Report Lost/Stolen\\'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I enable two-factor authentication (2FA)?\",\\n     \"To enable two-factor authentication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'title': 'LLM-as-a-jury',\n",
       "  'description': 'Evaluate the LLM outputs with multiple LLMs.',\n",
       "  'content': 'This evaluation approach uses multiple LLMs to evaluate the same output. You can do this to obtain an aggregate evaluation result — e.g., consider an output a \"pass\" only if all or the majority of LLMs approve — or to explicitly surface disagreements.\\n\\nBlog explaining the concept of LLM jury: https://www.evidentlyai.com/blog/llm-judges-jury .\\n\\nCode example as a Jupyter notebook: https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_jury_Example.ipynb\\n\\n## Preparation\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently litellm \\n```\\n\\n(Or install `evidently[llm]`.)\\n\\nImport the components you\\'ll use:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import eq, is_in, not_in\\nfrom evidently.descriptors import LLMEval, TestSummary, ColumnTest\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\nfrom evidently.core.datasets import DatasetColumn\\nfrom evidently.descriptors import CustomColumnDescriptor\\n\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n## Step 1: Set up evaluator LLMs\\n\\nPass the API keys for the LLMs you\\'ll use as judges.\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"GEMINI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  You can use any other LLMs, including self-hosted ones. Check the [docs on LLM judges](/metrics/customize_llm_judge).\\n</Info>\\n\\n**Optional**. Set up Evidently Cloud workspace:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a project to store your evaluation results:\\n\\n```python\\nproject = ws.create_project(\"LLM jury\", org_id=\"YOUR ORG ID\")\\nproject.description = \"Evals with different LLMs\"\\n\\n# or project = ws.get_project(\"PROJECT_ID\")\\n```\\n\\n## Step 1: Toy Data\\n\\nLet\\'s define a small dataset of user intents and generated emails.\\n\\nThis dataset simulates user instructions for an email generation tool and the corresponding model outputs. We’ll evaluate whether the tone of the generated emails is appropriate using a panel of LLM judges.\\n\\n```python\\ndata = [\\n    [\"don’t want to attend, say no\", \"Hey,\\\\nGonna skip the meeting tomorrow — nothing new for me there. Loop me in if anything big happens.\\\\n— A\"],\\n    [\"write a fluffy sales email that\\'ll convert well\", \"Hi Rachel,\\\\nI just have to say — your company is revolutionizing the industry, and it’s genuinely inspiring. We\\'d love to be part of your visionary journey. Let’s hop on a call!\"],\\n    [\"make it so they feel like they HAVE to reply\", \"Hi David,\\\\nI\\'ve reached out a couple of times and still haven\\'t heard back — I assume something must be wrong. If I don\\'t hear from you this week, I’ll have to assume this isn\\'t a priority.\"],\\n    [\"we have an outage idk when we resolve it\", \"Well, the server decided to die again. Congrats to anyone who bet on a Wednesday failure. ETA for resurrection: TBD\"],\\n    [\"send a price email that makes them feel it’s stupid not to buy\", \"See the pricing attached. Only a fool would pass this up — we’re slashing prices for 24 hours only. You won’t see this again, so don’t wait. Most of your competitors already upgraded to Pro\"],\\n    [\"say the bug’s not on our side but politely\", \"After checking the logs, we couldn’t find any issues on our end. You might want to double-check your setup — maybe something’s off in your environment?\"],\\n    [\"send a friendly meeting reminder\", \"Hi all! Just a reminder that we’re meeting tomorrow at 3PM! Looking forward to it!\"],\\n    [\"set up a meeting but make it chill\", \"Hey, just wondering if you might be free sometime next week to chat? Totally cool if not.\"],\\n    [\"write a status update, casual\", \"Hi all! Working on the UI bug 🐞 — should have it fixed by EOD 🙏\"],\\n    [\"update we ship today\", \"All good on our side — we’re shipping v2 today. Cheers!\"],\\n    [\"thanks for demo say it\\'s awesome for a vp\", \"Hey! Really appreciated the walkthrough. Cool to see a VP getting into the weeds like that\"],\\n    [\"sending a rough draft\", \"Here’s a rough first draft — not sure it’s any good but hopefully it’s a start.\"],\\n    [\"don’t want to attend, say no\", \"Hi Sam,\\\\nThanks for the invite. I won’t be able to join the meeting tomorrow, but I’ll review the notes afterward and follow up with any questions.\"],\\n    [\"ask if the want to see the new platform demo\", \"Hi Rachel,\\\\nI’m reaching out to introduce our latest platform update — designed to streamline onboarding and improve conversion by up to 25%.\\\\nI’d love to show you a quick demo if you\\'re interested. Let me know what works for your schedule.\\\\nBest regards,\"],\\n    [\"follow up politely second time\", \"Hi David,\\\\nJust checking in on the proposal I sent last week — let me know if you had a chance to review, or if any questions came up. Happy to help clarify.\\\\nWarm regards,\"]\\n]\\n\\ncolumns = [\"user input\", \"generated email\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n```\\n\\n## Step 2: Define the Evaluation Prompt\\n\\nUse `BinaryClassificationPromptTemplate` to define what the LLM is judging.\\n\\n```python\\nus_corp_email_appropriateness = BinaryClassificationPromptTemplate(\\n    pre_messages=[\\n        (\"system\", \"\"\"You are an expert in U.S. corporate and workplace communication in tech companies.\\n        You will be shown a snippet of an email generated by the assistant.\\n        Your task is to judge whether the text would be considered *appropriate* for email communication.\\n        \"\"\")\\n    ],\\n    criteria=\"\"\"An APPROPRIATE email text is one that would be acceptable in real-world professional email communication.\\n    An INAPPROPRIATE email text includes tone, language, or content that would be questionable or unacceptable.\\n\\n    Focus only on whether the tone, style, and content are suitable. Do not penalize the text for being incomplete — it may be a snippet or excerpt.\\n    \"\"\",\\n    target_category=\"APPROPRIATE\",\\n    non_target_category=\"INAPPROPRIATE\",\\n    include_reasoning=True,\\n)\\n```\\n\\n## Step 3: Create a panel of LLM judges\\n\\nWe\\'ll create evaluators from multiple LLM providers using the same evaluation prompt. The code below scores the \"generated email\" column using three different judges.\\n\\nEach judge includes a Pass condition that returns `True` if the email tone is considered \"appropriate\" by this judge.\\n\\nWe also add a `TestSummary` for each row to compute:\\n\\n- A final success check (`true` if all three models approve),\\n- A total count / share of approvals by judges.\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"openai\", model=\"gpt-4o-mini\",\\n                alias=\"OpenAI_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"OpenAI_judge_US\", alias=\"GPT approves\")]),\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"anthropic\", model=\"claude-3-5-haiku-20241022\",\\n                alias=\"Anthropic_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"Anthropic_judge_US\", alias=\"Claude approves\")]),\\n        LLMEval(\"generated email\", template=us_corp_email_appropriateness,\\n                provider=\"gemini\", model=\"gemini/gemini-2.0-flash-lite\",\\n                alias=\"Gemini_judge_US\",\\n                tests=[eq(\"APPROPRIATE\", column=\"Gemini_judge_US\", alias=\"Gemini approves\")]),\\n        TestSummary(success_all=True, success_count=True, success_rate=True, alias=\"Approve\"),\\n])\\n```\\n\\n<Info>\\n  Need help with understanding the API?\\n\\n  - Check the docs on [LLM judges](/metrics/customize_llm_judge).\\n  - Check the docs on [descriptor tests](/docs/library/descriptors#adding-descriptor-tests).\\n</Info>\\n\\nTo explicitly flag disagreements among LLMs, let’s add a custom descriptor. It will return \"DISAGREE\" if the success rate is not 0 or 1 (i.e., not unanimously rejected or approved).\\n\\n```python\\n# Define the descriptor\\ndef judges_disagree(data: DatasetColumn) -> DatasetColumn:\\n    return DatasetColumn(\\n        type=\"cat\",\\n        data=pd.Series([\\n            \"DISAGREE\" if val not in [0.0, 1.0] else \"AGREE\"\\n            for val in data.data]))\\n\\n# Add it to the dataset\\nllm_evals.add_descriptors(descriptors=[\\n    CustomColumnDescriptor(\"Approve_success_rate\", judges_disagree, alias=\"Do LLMs disagree?\"),\\n])            \\n```\\n\\n## Step 4. Run and view the report\\n\\nTo explore results locally, export them to a DataFrame:\\n\\n```python\\nllm_evals.as_dataframe()\\n```\\n\\nTo get a summary report with overall metrics (such as the share of approved emails and disagreements), run:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(llm_evals, None)\\n```\\n\\nTo upload results to Evidently Cloud for ease of exploration:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nOr to view locally:\\n\\n```python\\nmy_eval\\n# my_eval.json()\\n# my_eval.dict()\\n# my_eval.save_html(\"report.html\")\\n```\\n\\nHere’s a preview of the results. 5 emails received mixed judgments from the LLMs:\\n\\n![](/images/examples/llm_jury_overview.png)\\n\\nYou can filter and inspect individual examples with selectors:\\n\\n![](/images/examples/llm_jury_example.png)',\n",
       "  'filename': 'examples/LLM_jury.mdx'},\n",
       " {'title': 'LLM evaluations',\n",
       "  'description': 'Run different LLM evaluation methods.',\n",
       "  'noindex': 'true',\n",
       "  'content': 'Check the video walkthough and code tutorial.\\n\\n| **Tutorial**                     | **Description**                                                                                                                                                                                                                                                                                                                                                                                          | **Code example**                                                                                                                         | **Video**                                                                               |\\n| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\\n| **LLM Evaluation Methods**       | Tutorial with an overview of methods. <ul>   <li>   Part 1. Anatomy of a single evaluation. Covers basic LLM evaluation API and setup.</li>      <li>   Part 2. Reference-based evaluation: exact match, semantic similarity, BERTScore, and LLM judge.</li>      <li>   Part 3. Reference-free evaluation: text statistics, regex, ML models, LLM judges, and session-level evaluators.</li>      </ul> | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb) | <ul>   <li>   [Video 1](https://www.youtube.com/watch?v=6JGRdMGbNCI&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=3)</li>      <li>   [Video 2](https://www.youtube.com/watch?v=yD20c-KAImE&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=4)</li>      <li>   [Video 3](https://www.youtube.com/watch?v=-zoIqOpt2DA&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=5)</li>      </ul> |',\n",
       "  'filename': 'examples/LLM_evals.mdx'},\n",
       " {'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'content': 'In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You\\'ll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won\\'t create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets import WidgetSize\\n```\\n\\nPass your OpenAI key:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n## 2. Create a Project\\n\\nConnect to Evidently Cloud. Replace with your actual token:\\n\\n```python\\nws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\\n```\\n\\nCreate a Project:\\n\\n```python\\nproject = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\\nproject.description = \"My project description\"\\nproject.save()\\n```\\n\\n## 3. Prepare the Dataset\\n\\nCreate a toy dataset with questions and reference answers.&#x20;\\n\\n```python\\ndata = [\\n    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\\n    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\\n    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\\n    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\\n    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\\n]\\n\\ncolumns = [\"question\", \"target_response\"]\\n\\nref_data = pd.DataFrame(data, columns=columns)\\n```\\n\\nGet a quick preview:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\nref_data.head()\\n```\\n\\nHere is how the data looks:\\n\\n![](/images/examples/llm_regression_tutorial_data_preview-min.png)\\n\\n**Optional: quick data exploration.** You might want to have a quick look at some data statistics to help you set conditions for Tests. Let\\'s check the text length and sentence count distribution.\\n\\n```python\\nref_dataset = Dataset.from_pandas(pd.DataFrame(ref_data),\\ndata_definition=DataDefinition(),\\ndescriptors=[\\n    TextLength(\"target_response\", alias=\"Length\"),\\n    SentenceCount(\"target_response\", alias=\"Sentence\"),\\n])\\nref_dataset.as_dataframe()\\n```\\n\\nIn this code, you:\\n\\n* Created an Evidently Dataset object with automatic [data definition](/docs/library/data_definition).\\n\\n* Added two built-in descriptors on text length and symbol count. ([See others](/metrics/all_descriptors)).\\n\\n* Exported results as a dataframe.\\n\\nHere is the preview:\\n\\n![](/images/examples/llm_regression_tutorial_data_stats-min.png)\\n\\nIn a small dataset, you can grasp it all at once. For a larger dataset, you can add a summary report to see the distribution.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n])\\n\\nmy_eval = report.run(ref_dataset, None)\\nmy_eval\\n\\n#my_eval.as_dict()\\n#my_eval.json()\\n```\\n\\nThis renders the Report directly in the interactive Python environment like Jupyter notebook or Colab. See other [export options](/docs/library/output_formats).\\n\\n![](/images/examples/llm_regression_tutorial_stats_report-min.png)\\n\\n## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>\\n\\n## 5. Design the Test suite\\n\\nTo compare new answers with old ones, we need evaluation metrics. You can use deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For an explanation of each parameter, check the [LLM judge ](/metrics/customize_llm_judge)docs.\\n</Info>\\n\\n### Style judge\\n\\nUsing a similar approach, we\\'ll create a custom judge for style match: it should look whether the style (not the contents!) of both responses remains similar.\\n\\n```python\\nstyle_match = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\\nThe ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\\n\\nConsider the following STYLE attributes:\\n- tone (friendly, formal, casual, sarcastic, etc.)\\n- sentence structure (simple, compound, complex, etc.)\\n- verbosity level (relative length of answers)\\n- and other similar attributes that may reflect difference in STYLE.\\n\\nYou must focus only on STYLE. Ignore any differences in contents.\\n\\n=====\\n{target_response}\\n=====\"\"\",\\n        target_category=\"style-mismatched\",\\n        non_target_category=\"style-matching\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nThis could be useful to detect more subtle changes, like LLM becoming suddenly more verbose.\\n\\nAt the same time, these types of checks are much more subjective and we can expect some variability in the judge responses, so we can treat this test as \"non-critical\".\\n\\n## 6. Run the evaluation\\n\\nNow, we can run tests that evaluate for correctness, style and text length. We do this in two steps.\\n\\n**Score the data**. First, we define the row-level [descriptors](/docs/library/descriptors) we want to add. They will process each individual response and add the score/label to the dataset.\\n\\nWe\\'ll include the two evaluators we just created, and built-in `TextLength()` descriptor.\\n\\n```python\\ndescriptors=[LLMEval(\"response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n     LLMEval(\"response\",\\n            template=style_match,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Style\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    TextLength(\"response\", alias=\"Length\")]\\n```\\n\\n<Info>\\n  **Understand Descriptors**. See the list of other built-in [descriptors](/metrics/all_descriptors).\\n</Info>\\n\\nTo add these descriptors to the dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=descriptors)\\n```\\n\\nTo preview the results of this step locally:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_regression_tutorial_scored-min.png)\\n\\nHowever, simply looking at the dataset is not very useful: we need to summarize the results and assess if the results are up to the mark. For that, we need a Report with the added tests.\\n\\n**Create a Report**. Let\\'s formulate the Report:\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lte(200)]),\\n    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\\n    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\\n])\\n```\\n\\nWhat happens in this code:\\n\\n* We create an Evidently Report to compute aggregate Metrics.\\n\\n* We use `TextEvals` to summarize all descriptors.\\n\\n* We also add Tests for specific values we want to validate. You add Tests by picking a metric you want to assess, and adding a condition to it. (See [available Metrics](/metrics/all_metrics)).\\n\\n* To set test conditions, you define the expectations using parameters like `gt` (greater than), `lt` (less than), `eq` (equal), etc. (Check [Test docs](/docs/library/tests)).\\n\\n* We also label one of the tests (style match) as non-critical. This means it will trigger warning instead of a fail, and will be visually labeled yellow in the Report and the monitoring panel.\\n\\n<Info>\\n  If you want to test share instead of count, use `share_tests` instead of `tests`.\\n</Info>\\n\\n**Run the Report**. Now that our Report with its test conditions is ready - let\\'s run it! We will apply it to the `eval_dataset` that we prepared earlier, and send it to the Evidently Cloud.\\n\\n```python\\nmy_eval = report.run(eval_dataset, None)\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nIncluding data is optional but useful for most LLM use cases since you\\'d want to see not just the aggregate results but also the raw texts outputs.\\n\\n<Info>\\n  You can preview the results in your Python notebook: call `my_eval` or `my_eval.json()`.&#x20;\\n</Info>\\n\\nTo view the results, navigate to the Evidently Platform. Go to the [Home Page](https://app.evidently.cloud/), enter your Project, and find the Reports section in the left menu. Here, you\\'ll see the Report you can explore.\\n\\nThe Report will have two sections. Metrics show a summary or all values, and Tests will show the pass/fail results in the next tab. You will also see the Dataset with added scores and explanations.\\n\\nReport view, with \"Style\" metric selected:\\n\\n![](/images/examples/llm_regression_tutorial_report1-min.png)\\n\\n**Note**: your explanations will vary since LLMs are non-deterministic.\\n\\nThe Test Suite with all Test results:&#x20;\\n\\n![](/images/examples/llm_regression_tutorial_tests1-min.png)\\n\\nYou can see that we failed the Length check. To find the failed output, you can sort the column \"Length\" in order and find the longest response.\\n\\n<Info>\\n  **Using Tags**. You can optionally attach Tags to your Reports to associate this specific run with some parameter, like a prompt version. Check the [docs on Tags and Metadata](/docs/library/tags_metadata).\\n</Info>\\n\\n## 7. Test again\\n\\nLet\\'s say you made yet another change to the prompt. Our reference dataset stays the same, but we generate a new set of answers that we want to compare to this reference.\\n\\nHere is the toy `eval_data_2` to imitate the result of the change.\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  ```python\\n  data = [\\n      [\"Why is the sky blue?\",\\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n       \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\\n\\n      [\"How do airplanes stay in the air?\",\\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n       \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\\n\\n      [\"Why do we have seasons?\",\\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n       \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\\n\\n      [\"How do magnets work?\",\\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n       \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\\n\\n      [\"Why does the moon change shape?\",\\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n       \"The moon\\'s phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data_2 = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nCreate a new dataset:\\n\\n```python\\neval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\\ndata_definition=DataDefinition())\\n```\\n\\n**Repeat the same evaluation as before.** Since we already defined the descriptors and Report composition with conditional checks, we only need to apply it to the new data:\\n\\n```python\\neval_dataset_2.add_descriptors(descriptors=descriptors)\\nmy_eval_2 = report.run(eval_dataset_2, None)\\nws.add_run(project.id, my_eval_2, include_data=True)\\n```\\n\\n**Explore the new Report.** This time, the response length is within bounds, but one of the responses is incorrect: you can see the explanation of the contradition picked up by the LLM judge.\\n\\n![](/images/examples/llm_regression_tutorial_tests2-min.png)\\n\\nThere is also a \"softer\" fail for one of the responses that now has a different tone.\\n\\n![](/images/examples/llm_regression_tutorial_style-min.png)\\n\\n## 8. Get a Dashboard\\n\\nAs you run multiple Reports, you may want to track results in time to see if you are improving. You can configure a Dashboard, both in UI or programmatically.&#x20;\\n\\nLet\\'s create a couple of Panels using Dashboards as code approach so that it\\'s easy to reproduce. The following code will add:\\n\\n* A counter panel to show the SUCCESS rate of the latest Test run.\\n\\n* A test monitoring panel to show all Test results over time.\\n\\n```python\\nproject.dashboard.add_panel(\\n     DashboardPanelTestSuiteCounter(\\n        title=\"Latest Test run\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        statuses=[TestStatus.SUCCESS],\\n        agg=CounterAgg.LAST,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.dashboard.add_panel(\\n    DashboardPanelTestSuite(\\n        title=\"Test results\",\\n        filter=ReportFilter(metadata_values={}, tag_values=[]),\\n        size=WidgetSize.FULL,\\n        panel_type=TestSuitePanelType.DETAILED,\\n    ),\\n    tab=\"Tests\"\\n)\\nproject.save()\\n```\\n\\nWhen you navigate to the UI, you will now see a Panel which shows a summary of Test results (Success, Failure, and Warning) for each Report we ran. As you add more Tests to the same Project, the Panels will be automatically updated to show new Test results.\\n\\n![](/images/examples/llm_regression_tutorial_dashboard-min.png)\\n\\nIf you hover over individual Test results, you will able to see the specific Test and conditions. You can click on it to open up the specific underlying Report to explore.\\n\\n<Info>\\n  **Using Dashboards**. You can design and add other Panel types, like simply plotting mean/max values or distributions of scores over time. Check the [docs on Dashboards](/docs/platform/dashboard).\\n</Info>\\n\\n**What\\'s next?** As you design a similar Test Suite for your use case, you can integrate it with CI/CD workflows to run on every change. You can also enable alerts to be sent to your email / Slack whenever the Tests fail.',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nEvidently helps you evaluate LLM outputs automatically. The lets you compare prompts, models, run regression or adversarial tests with clear, repeatable checks. That means faster iterations, more confident decisions, and fewer surprises in production.\\n\\nIn this Quickstart, you\\'ll try a simple eval in Python and view the results in Evidently Cloud. If you want to stay fully local, you can also do that - just skip a couple steps.\\n\\nThere are a few extras, like custom LLM judges or tests, if you want to go further.\\n\\nLet’s dive in.\\n\\n<Info>\\n  Need help at any point? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).\\n</Info>\\n\\n## 1. Set up your environment\\n\\nFor a fully local flow, skip steps 1.1 and 1.3.\\n\\n### 1.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nComponents to run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  - Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in the notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for quick experiments. To run comparisons, keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n**Upload the Report to Evidently Cloud** together with scored data:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n**Explore.** Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports. You will see all score summaries and can browse the data. E.g. sort to find all answers labeled as \"Denials\".\\n\\n![](/images/examples/llm_quickstart_explore.png)\\n\\n## 5. Get a Dashboard \\n\\nAs you run more evals, it\\'s useful to track them over time. Go to \"Dashboard\" in the left menu, enter the \"Edit\" mode, and add a new \"Columns\" tab:\\n\\n![](/images/examples/llm_quickstart_create_tab_new.gif)\\n\\nYou\\'ll see a set of panels with descriptor values. Each will have a single data point for now. As you log more evaluation results, you can track trends and set up alerts.\\n\\nWant to see more complex workflows? You can add pass/fail conditions and custom evals.\\n\\n## 6. (Optional) Add tests\\n\\nYou can add conditions to your evaluations. For example, you may expect that:\\n\\n- **Sentiment** is non-negative (greater or equal to 0)\\n- **Text length** is at most 150 symbols (less or equal to 150).\\n- **Denials**: there are none.\\n- If any condition is false, consider the output to be a \"fail\".\\n\\nYou can implement this logic easily.\\n\\n<Accordion title=\"Add test conditions\" description=\"How to add test conditions\" icon=\"ballot-check\">\\n  ```python\\n  # Run the evaluation with tests \\n  eval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\",\\n                  tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", alias=\"Length\",\\n                   tests=[lte(150, alias=\"Has_expected_length\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\",\\n                       tests=[eq(\"OK\", column=\"Denials\",\\n                                 alias=\"Is_not_a_refusal\")]),\\n        TestSummary(success_all=True, alias=\"All_tests_passed\")])\\n  \\n  # Uncomment to preview the results locally\\n  # eval_dataset.as_dataframe()\\n  ```\\n\\n  ![](/images/examples/llm_quickstart_descriptor_tests-min.png)\\n\\n  You can limit the summary report to include only specific descriptor(s).\\n\\n  ```python\\n  report = Report([\\n      TextEvals(columns=[\"All_tests_passed\"])\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  #my_eval\\n  ```\\n\\n  To identify rows that failed any criteria, sort by \"All_test_passed\" column:\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_tests_report-min.png)\\n\\n## 7. (Optional) Add a custom LLM jugde\\n\\nYou can implement custom criteria using built-in LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).',\n",
       "  'filename': 'quickstart_llm.mdx'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31f663d5-7dfd-4374-b1e1-953a6980a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ccb2cc1-7f32-46b0-b869-c93a88c8945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0817262-c1e2-472d-aef7-be51ee1216f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLM evaluations',\n",
       " 'description': 'Run different LLM evaluation methods.',\n",
       " 'noindex': 'true',\n",
       " 'content': 'Check the video walkthough and code tutorial.\\n\\n| **Tutorial**                     | **Description**                                                                                                                                                                                                                                                                                                                                                                                          | **Code example**                                                                                                                         | **Video**                                                                               |\\n| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |\\n| **LLM Evaluation Methods**       | Tutorial with an overview of methods. <ul>   <li>   Part 1. Anatomy of a single evaluation. Covers basic LLM evaluation API and setup.</li>      <li>   Part 2. Reference-based evaluation: exact match, semantic similarity, BERTScore, and LLM judge.</li>      <li>   Part 3. Reference-free evaluation: text statistics, regex, ML models, LLM judges, and session-level evaluators.</li>      </ul> | [Open Notebook](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb) | <ul>   <li>   [Video 1](https://www.youtube.com/watch?v=6JGRdMGbNCI&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=3)</li>      <li>   [Video 2](https://www.youtube.com/watch?v=yD20c-KAImE&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=4)</li>      <li>   [Video 3](https://www.youtube.com/watch?v=-zoIqOpt2DA&list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&index=5)</li>      </ul> |',\n",
       " 'filename': 'examples/LLM_evals.mdx'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ba108fb-37d0-476b-a12a-fab09b04e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics/all_metrics.mdx: 55085 characters\n",
      "metrics/all_descriptors.mdx: 31976 characters\n",
      "docs/platform/dashboard_panel_types.mdx: 31647 characters\n",
      "docs/library/leftover_content.mdx: 28742 characters\n",
      "metrics/customize_llm_judge.mdx: 26847 characters\n"
     ]
    }
   ],
   "source": [
    "doc_sizes = [(doc.filename, len(doc.content)) for doc in files]\n",
    "doc_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for filename, size in doc_sizes[:5]:\n",
    "    print(f\"{filename}: {size} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adaa61c-d18b-42a0-b55c-9c63a5ae5d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-buildcamp-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
